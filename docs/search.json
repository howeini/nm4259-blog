[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello, I’m Wei Ni!",
    "section": "",
    "text": "LinkedIn\n  \n  \n     GitHub\n  \n  \n     Email\n  \n\n  \n  \n\n\nA 4th-year student pursuing a double degree in Business Administration & Communications and New Media, with a minor in Interactive Media Development.\n\n\n\nPractical Figma skills - from basic wireframing to interactive prototyping\nMobile design workflows - industry approaches to interface development\nPortfolio-quality projects that show my design thinking process"
  },
  {
    "objectID": "index.html#what-i-hope-to-learn",
    "href": "index.html#what-i-hope-to-learn",
    "title": "Hello, I’m Wei Ni!",
    "section": "",
    "text": "Practical Figma skills - from basic wireframing to interactive prototyping\nMobile design workflows - industry approaches to interface development\nPortfolio-quality projects that show my design thinking process"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Week 3 - Individual Reflection #1",
    "section": "",
    "text": "Meeting Agenda: To finalise our choice of mobile/web asset\n\nMy group began by brainstorming potential options before coming together to refine our ideas. Since I use mobile applications more frequently than web interfaces, I naturally gravitated toward them. From there, I shortlisted a few familiar apps:\n\nMy everyday tools, Google Maps and Google Calendar\nA staple for scheduling meetings in University, when2meet\n\nReflecting on when2meet was particularly eye-opening. I had always criticised it for its outdated UI, but through a more critical lens, I realised how effective it actually is. With all necessary inputs displayed on a single page, the experience is almost effortless.\n\n\n\nLanding Page of the when2meet Interface\n\n\nThis reminded me that good design is not just about aesthetics, but about enabling users to achieve their goals efficiently. Still, I noted its limitations like the lack of time zone support, which highlighted how small gaps can become critical barriers in certain contexts.\nThe strongest mobile application evaluation I brought to the meeting was NUSmart Dining, launched in 2022 to reduce canteen crowd congestion at peak timings. As a frequent user, I had already noticed areas where it could be improved. To evaluate it systematically, I deliberately navigated through each page (beyond onboarding, since I was already logged in) and reflected on the typical actions I perform as a user. Norman’s heuristics provided a useful framework to make sense of my observations — for instance, noticing when the app lacked visibility of system status or when affordances were unclear.\n\n\n\nMy evaluation of the NUSmart Dining app\n\n\nOne challenge I faced during the meeting was the distinction between Mobile Interaction Design (IxD) and User Experience (UX) Design. As we presented our ideas, I felt that many of our points were framed from the perspective of UX, as they revolved around how the user feels while using the app and the overall satisfaction of the experience. This left me questioning how this module differed from NM3243 User Experience Design.\nTo clarify my doubts, I did further reading about the differences between the two. I began to understand that IxD has a narrower but more technical focus on how the system behaves in response to user inputs, and how clearly those responses guide users through the journey. Whereas, UX considers the entire user journey. It is important to note that IxD contributes to UX, similar to a subset.\n\n\n\nInteraction Design vs UX Design: What is the Difference?\n\n\nThis distinction shifted how I approached evaluating the NUSmart Dining app. Instead of framing my frustrations purely as usability issues, I began pinpointing where the interactions themselves could be improved. This reframing reminded me that as a designer, I should understand the mechanics of interaction that either hinder or empower the user in achieving their goal.\nMoving forward, instead of just noticing that users are frustrated, I want to ask myself: Am I distinguishing between what I find “annoying” versus what actually breaks an interaction for the user? Does this critique suggest an actionable design change, or is it simply an observation? I believe this can help mature my design thinking and process.\n\n\n\n\n\n\nRelevant Articles\n\n\n\n\nWhen2Meet vs. LettuceMeet: a case study in UI and aesthetics\nUX vs IA vs UI vs IxD design differences (& what they stand for)\nWhat is the difference between Interaction Design and UX Design?\nInteraction Design vs UX: What’s the Difference?\nUI Design vs. UX Design vs. Interaction Design vs. Visual Design: How Do They Differ?"
  },
  {
    "objectID": "posts/blogpost2/index.html",
    "href": "posts/blogpost2/index.html",
    "title": "Week 4 - Individual Reflection #2",
    "section": "",
    "text": "Our group decided on two user research methods: contextual inquiry (CI) and a survey. The survey is currently ongoing, but ideally the data would substantiate quantitatively our insights from CI and identify the most pressing pain point to tackle in our design.\nWhy Contextual Inquiry?\nIt allows us to observe app usage in a natural setting and provides opportunities for clarifications, unlike the Fly On the Wall method. Personally, I feel that users would feel more relaxed narrating in the context of CI as opposed to the Think Aloud method, as the master/apprentice model encourages narration organically. \nMy Experience\nWhat I found especially important was remembering that we are observing users as they naturally used the app, letting the process unfold on its own terms. Interestingly, the session was what the user described as a “failure”, which in hindsight was revealing—it surfaced details I might have missed and showed how “failed” experiences often expose the most useful insights.\n\nThe user ordered TehO on the bus to COM3 despite a &gt;15 min wait warning, kept refreshing the app which stayed on “Preparing”. After 15 minutes of watching in-person orders fulfilled faster, they only got their drink after they approached the vendor.\n\nThis experience challenged several of my assumptions.\nFirst, I had assumed that app usage began only at the canteen itself. Yet this user started their journey much earlier, while still on the bus. That made me realise how important the mobile context really is—decisions often begin before on-site arrival. I had also thought users might struggle with recognising canteen names, but for this participant, familiarity from repeated visits meant this step was effortless. Recognition, I realised, depends less on app experience and more on campus familiarity.\n\n\n\nAnalysis of Ordering Process\n\n\nKeeping a user-centric lens while examining how they interacted with features also revealed several insights. Information presented, such as the number of orders ahead, was not internalised—the user saw the data but could not translate it into what it meant for their own wait time. Checkout, too, felt like muscle memory, an almost unconscious tap rather than a deliberate decision.\n\n\n\nAnalysis of Checkout Process\n\n\nThe most striking issue was inconsistent system feedback. Although the order was confirmed, post-payment there were no real-time updates. The status remained stuck at “Preparing”, with no signs of progress (e.g., orders ahead reducing), leaving the user anxious and perceiving the order as a “failure”. This lack of closure and visibility amplified their anxiety. Yet, despite all this, the distrust seemed situational. They framed it as a “first-time” issue, suggesting their overall trust in the system remained intact.\n\n\n\nAnalysis of Collection Process\n\n\nUpon reflection, limitations exist for our study. Most of our observations took place during peak hours, and only at a few canteen locations. All our participants were students, excluding staff and public users, and most orders we observed were single-item purchases. While time pressure definitely shaped who we were able to recruit for CI, I am hopeful that the survey will broaden the scope of our findings!"
  },
  {
    "objectID": "posts/blogpost2/index.html#assumptions-challenged",
    "href": "posts/blogpost2/index.html#assumptions-challenged",
    "title": "Week 4 - Individual Reflection #2",
    "section": "Assumptions Challenged",
    "text": "Assumptions Challenged\n\n\n\n\n\n\n\n\n\n\nAssumption\nReality\nImplication\n\n\n\n\nStarting point of user journey\nApp usage begins only at the canteen\nUser began their order while still on the bus to COM3\nApp usage extends beyond physical canteen, reinforcing the mobile context\n\n\nCanteen recognition\nUsers may struggle finding intended canteens due to limited knowledge on their names\nUsers could already be familiar due to habitual visits\nCanteen recognition depends on campus familiarity, not necessarily app experience"
  },
  {
    "objectID": "posts/blogpost2/index.html#key-insights",
    "href": "posts/blogpost2/index.html#key-insights",
    "title": "Week 4 - Individual Reflection #2",
    "section": "Key Insights",
    "text": "Key Insights\n\nInformation is not internalised. The user noticed data presented (like orders ahead) but was unable to translate the information into a personal impact, suggesting a misalignment between the interface design and the user's mental model.\nCheckout was automatic. Tapping \"Checkout\" felt like unconscious muscle memory rather than an intentional decision, highlighting how some steps in the flow have become habitual.\nSystem feedback was inconsistent and insufficient. This order was perceived as a 'failure': experiencing extremely long wait times (, having to approach the vendor directly, and still seeing the order status stuck at 'Preparing' even a day later with no completion notification. This lack of clear feedback and visibility into order progress amplified the user's existing anxiety.\nDistrust in the system was situational. The negative experience was framed as a \"first-time\" issue, suggesting that despite user confidence being undermined, overall trust in the system remains intact.\n\nWhile data collection remains incomplete, these are potential issues I foresee:\n\nOur insights so far are drawn mostly from peak-hour usage.\nObservations are limited to a few canteen locations.\nOur participants have all been students, neglecting staff and public.\nOrders observed were mostly single-item purchases.\n\nThese constraints may narrow the scope of our findings, and future data collection should aim for greater diversity in contexts and user types."
  },
  {
    "objectID": "posts/wireframe/index.html",
    "href": "posts/wireframe/index.html",
    "title": "Week 5 - Individual Reflection #3",
    "section": "",
    "text": "Our group decided to focus on the pain point of order visibility, particularly the vague status of “Preparing” that leaves the waiting period between preparation and collection unclear. To better understand this, we pulled up the relevant interface screens and analysed them using Nielsen’s heuristics.\n\n\n\nAnalysis of Relevant Interface Screens\n\n\nAt first, I was overly critical. This was useful to empathise with users’ frustrations, but it quickly became reductive when I found myself suggesting that features be removed simply because they seemed “useless” or redundant. In hindsight, this was a flattening approach. The fact that the designers chose to include that information meant it must have value for someone. The real challenge is not replacing things, but figuring out how to elevate what was already there and rework information to be more meaningful and digestible.\nThat shift in mindset helped me recognise the importance of context and stakeholders. For example, on the receipt screen, order numbers and details felt irrelevant at first—why remind users of what they already know? But thinking through the collection stage, it became clear this information is critical for vendors. What matters is not just what is displayed, but to whom and at what point in time. Our navigation flow exercise also reinforced this, since we had to consider multiple scenarios: whether an active order exists, who is viewing the screen, and what their priorities are at that moment.\nI also realised how much language and framing shape the way users interpret their own actions. Take the ‘Order Again’ button: its confirmation prompt asks, ‘Are you sure to order the same item again?’. This phrasing implicitly frames repetition as a mistake rather than a convenience, subtly undermining user confidence. It reminded me that text is never neutral — it encodes assumptions about intent and directs how users perceive their choices.\n\n\n\n‘Order Again’ Interface Screens\n\n\nI felt relieved we were only producing the wireframe, as the weight of such micro-decisions was overwhelming. But this also reinforced the value of starting with low-fidelity prototypes: they allow me to focus on structure and flow first, ensuring a solid foundation before layering in details like tone and phrasing, which carry disproportionate influence on the final user experience.\nAnother important lesson came from examining technical constraints. At first, I dismissed the restriction of not being able to order across stalls as a poor design choice. But deeper analysis of our data showed that most users make single-item purchases, revealing how this constraint reflects actual usage patterns. Perhaps, good design is not always about maximising flexibility for edge cases, but about recognising trade-offs and optimising for the most common, meaningful interactions.\nOverall, my biggest learning was to avoid designing from a top-down, idealised vision. It is not about copying what works elsewhere or imposing what I think is “good.” Instead, I should work bottom-up, by starting from what exists, understanding the rationale behind it, and improving it in context. Iterative design becomes less about replacing and more re-framing and aligning information with the needs of different users at different points in their journey.\n\n\n\nSummary of Our Discussion So Far"
  },
  {
    "objectID": "posts/user_research/index.html",
    "href": "posts/user_research/index.html",
    "title": "Week 4 - Individual Reflection #2",
    "section": "",
    "text": "Our group decided on two user research methods: contextual inquiry (CI) and a survey. The survey is currently ongoing, but ideally the data would substantiate quantitatively our insights from CI and identify the most pressing pain point to tackle in our design.\nWhy Contextual Inquiry?\nIt allows us to observe app usage in a natural setting and provides opportunities for clarifications, unlike the Fly On the Wall method. Personally, I feel that users would feel more relaxed narrating in the context of CI as opposed to the Think Aloud method, as the master/apprentice model encourages narration organically. \nMy Experience\nWhat I found especially important was remembering that we are observing users as they naturally used the app, letting the process unfold on its own terms. Interestingly, the session was what the user described as a “failure”, which in hindsight was revealing—it surfaced details I might have missed and showed how “failed” experiences often expose the most useful insights.\n\nThe user ordered TehO on the bus to COM3 despite a &gt;15 min wait warning, kept refreshing the app which stayed on “Preparing”. After 15 minutes of watching in-person orders fulfilled faster, they only got their drink after they approached the vendor.\n\nThis experience challenged several of my assumptions.\nFirst, I had assumed that app usage began only at the canteen itself. Yet this user started their journey much earlier, while still on the bus. That made me realise how important the mobile context really is—decisions often begin before on-site arrival. I had also thought users might struggle with recognising canteen names, but for this participant, familiarity from repeated visits meant this step was effortless. Recognition, I realised, depends less on app experience and more on campus familiarity.\n\n\n\nAnalysis of Ordering Process\n\n\nKeeping a user-centric lens while examining how they interacted with features also revealed several insights. Information presented, such as the number of orders ahead, was not internalised—the user saw the data but could not translate it into what it meant for their own wait time. Checkout, too, felt like muscle memory, an almost unconscious tap rather than a deliberate decision.\n\n\n\nAnalysis of Checkout Process\n\n\nThe most striking issue was inconsistent system feedback. Although the order was confirmed, post-payment there were no real-time updates. The status remained stuck at “Preparing”, with no signs of progress (e.g., orders ahead reducing), leaving the user anxious and perceiving the order as a “failure”. This lack of closure and visibility amplified their anxiety. Yet, despite all this, the distrust seemed situational. They framed it as a “first-time” issue, suggesting their overall trust in the system remained intact.\n\n\n\nAnalysis of Collection Process\n\n\nUpon reflection, limitations exist for our study. Most of our observations took place during peak hours, and only at a few canteen locations. All our participants were students, excluding staff and public users, and most orders we observed were single-item purchases. While time pressure definitely shaped who we were able to recruit for CI, I am hopeful that the survey will broaden the scope of our findings!"
  }
]